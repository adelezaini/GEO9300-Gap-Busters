{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to fill observed meteorological data with Met Nordic gridded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing out MICE for gap-filling using Met Nordic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Temperature  Humidity  WindSpeed\n",
      "0         15.0      80.0        5.0\n",
      "1         14.2      78.0        NaN\n",
      "2          NaN       NaN        NaN\n",
      "3          NaN       NaN        NaN\n",
      "4         17.0      90.0        5.1\n",
      "5          NaN       NaN        4.5\n",
      "6         13.1      75.0        3.8\n",
      "7         12.5       NaN        4.0\n",
      "8         14.8      82.0        NaN\n",
      "9          NaN      81.0        4.3\n",
      "\n",
      "Imputed Data:\n",
      "   Temperature   Humidity  WindSpeed\n",
      "0    15.000000  80.000000   5.000000\n",
      "1    14.200000  78.000000   4.473759\n",
      "2    14.464116  79.556835   4.429090\n",
      "3    14.464116  79.556835   4.429090\n",
      "4    17.000000  90.000000   5.100000\n",
      "5    14.478003  79.353229   4.500000\n",
      "6    13.100000  75.000000   3.800000\n",
      "7    12.500000  71.101454   4.000000\n",
      "8    14.800000  82.000000   4.258958\n",
      "9    14.634923  81.000000   4.300000\n"
     ]
    }
   ],
   "source": [
    "# test example:\n",
    "\n",
    "data = {\n",
    "    'Temperature': [15.0, 14.2, np.nan, np.nan, 17.0, np.nan, 13.1, 12.5, 14.8, np.nan],\n",
    "    'Humidity': [80, 78, np.nan, np.nan, 90, np.nan, 75, np.nan, 82, 81],\n",
    "    'WindSpeed': [5.0, np.nan, np.nan, np.nan, 5.1, 4.5, 3.8, 4.0, np.nan, 4.3]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize MICE Imputer\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "# Perform imputation\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "\n",
    "print(\"\\nImputed Data:\")\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load point observation data\n",
    "df = pd.read_csv(\"../data/Tuddal_data.csv\", index_col=0, na_values=np.nan)\n",
    "\n",
    "# Load NetCDF reanalysis data\n",
    "nc = xr.open_dataset(\"../data/met_analysis_1_0km_nordic_v3_yr_20161108_20240528_formatSURFEXnewTuddal.nc\")\n",
    "\n",
    "# Specify variables of interest from each dataset (original names)\n",
    "vars_df = [\"precip_int_h_D\", \"air_pressure\", \"wind_dir\", \"RH\", \"specific_humidity\",\n",
    "           \"wind_speed\", \"R_LW_in_corr\", \"R_SW_in\", \"air_temperature\"]\n",
    "\n",
    "vars_nc = list(nc.keys())\n",
    "vars_nc_to_remove = ['Rainf', 'Snowf', 'FRC_TIME_STP', 'LON', 'LAT', 'ZS', 'SCA_SWdown', 'CO2air', 'ZREF', 'UREF']\n",
    "vars_nc = list(set(vars_nc) - set(vars_nc_to_remove))\n",
    "\n",
    "# Extract data from NetCDF file and reset index\n",
    "nc_data = nc[vars_nc].to_dataframe().reset_index()\n",
    "df = df[vars_df].reset_index()\n",
    "\n",
    "# Resample the NetCDF data to 30-minute intervals by interpolating\n",
    "nc_data_resampled = nc_data.set_index('time').resample('30min').interpolate(method='linear').reset_index()\n",
    "\n",
    "# Define the new column names based on your naming convention\n",
    "rename_mapping_df = {\n",
    "    \"precip_int_h_D\": \"precipitation\",\n",
    "    \"air_pressure\": \"pressure\",\n",
    "    \"wind_dir\": \"wind_direction\",\n",
    "    \"RH\": \"relative_humidity\",\n",
    "    \"specific_humidity\": \"humidity\",\n",
    "    \"wind_speed\": \"wind_speed\",\n",
    "    \"R_LW_in_corr\": \"longwave_radiation\",\n",
    "    \"R_SW_in\": \"shortwave_radiation\",\n",
    "    \"air_temperature\": \"temperature\"\n",
    "}\n",
    "\n",
    "rename_mapping_nc = {\n",
    "    original: new + '_nc'\n",
    "    for original, new in rename_mapping_df.items()\n",
    "}\n",
    "\n",
    "# Rename columns in point observation DataFrame\n",
    "df.rename(columns=rename_mapping_df, inplace=True)\n",
    "\n",
    "# Rename columns in resampled NetCDF DataFrame\n",
    "nc_data_resampled.rename(columns=rename_mapping_nc, inplace=True)\n",
    "\n",
    "# Merge point observations with NetCDF data on the time index\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "nc_data_resampled['time'] = pd.to_datetime(nc_data_resampled['time'])\n",
    "merged_df = pd.merge(df, nc_data_resampled, left_on='timestamp', right_on='time', suffixes=('_point', '_nc'))\n",
    "\n",
    "# Drop the time column extracted from NetCDF as the 'timestamp' column serves the same purpose\n",
    "merged_df.drop(columns=['time'], inplace=True)\n",
    "\n",
    "# Update impute_vars to use the new column names\n",
    "impute_vars = list(rename_mapping_df.values()) + list(rename_mapping_nc.values())\n",
    "\n",
    "print(\"Column names of merged DataFrame after renaming:\")\n",
    "print(merged_df.columns)\n",
    "\n",
    "# Continue with scaling and imputation as before\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(merged_df[impute_vars])\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "imputed_data = imputer.fit_transform(scaled_data)\n",
    "\n",
    "imputed_data = scaler.inverse_transform(imputed_data)\n",
    "\n",
    "for i, var in enumerate(impute_vars):\n",
    "    merged_df[var] = imputed_data[:, i]\n",
    "\n",
    "# Separate point observations and reanalysis data if needed\n",
    "df_filled = merged_df[['timestamp'] + list(rename_mapping_df.values())]\n",
    "nc_filled = merged_df[['timestamp'] + list(rename_mapping_nc.values())]\n",
    "\n",
    "print(\"Imputed Point Observations Data:\")\n",
    "print(df_filled.head())\n",
    "\n",
    "print(\"\\nImputed NetCDF Data:\")\n",
    "print(nc_filled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load point observation data\n",
    "df = pd.read_csv(\"../data/Tuddal_data.csv\", index_col=0, na_values=np.nan)\n",
    "\n",
    "# Load NetCDF reanalysis data\n",
    "nc = xr.open_dataset(\"../data/met_analysis_1_0km_nordic_v3_yr_20161108_20240528_formatSURFEXnewTuddal.nc\")\n",
    "\n",
    "# Specify variables of interest from each ds\n",
    "vars_df = [\"precip_int_h_D\", \"air_pressure\", \"wind_dir\", \"RH\", \"specific_humidity\",\n",
    "           \"wind_speed\", \"R_LW_in_corr\", \"R_SW_in\", \"air_temperature\"]\n",
    "\n",
    "vars_nc = list(nc.keys())\n",
    "vars_nc_to_remove = ['Rainf', 'Snowf', 'FRC_TIME_STP', 'LON', 'LAT', 'ZS', 'SCA_SWdown', 'CO2air', 'ZREF', 'UREF']\n",
    "vars_nc = list(set(vars_nc) - set(vars_nc_to_remove))\n",
    "\n",
    "nc_data = nc[vars_nc].to_dataframe().reset_index()\n",
    "df = df[vars_df].reset_index()\n",
    "\n",
    "# Resample the NetCDF data to 30-minute intervals by interpolating\n",
    "nc_data_resampled = nc_data.set_index('time').resample('30min').interpolate(method='linear').reset_index()\n",
    "\n",
    "# rename both ds to the following:\n",
    "new_var_names = [\"precipitation\", \"air_pressure\", \"wind_dir\", \"RH\", \"specific_humidity\",\n",
    "           \"wind_speed\", \"LW_in\", \"SW_in\", \"air_temperature\"]\n",
    "\n",
    "# rename ds:\n",
    "\n",
    "\n",
    "# Merge point observations with NetCDF data on the time index\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "nc_data['time'] = pd.to_datetime(nc_data['time'])\n",
    "merged_df = pd.merge(df, nc_data_resampled, left_on='timestamp', right_on='time', suffixes=('_point', '_nc'))\n",
    "\n",
    "# Drop the time column extracted from NetCDF as the 'timestamp' column serves the same purpose\n",
    "merged_df.drop(columns=['time'], inplace=True)\n",
    "\n",
    "# Selecting only the relevant variables for imputation\n",
    "impute_vars = vars_df + [var + '_nc' for var in vars_df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['RH', 'precip_int_h_D_nc', 'air_pressure_nc', 'wind_dir_nc', 'specific_humidity_nc', 'wind_speed_nc', 'R_LW_in_corr_nc', 'R_SW_in_nc', 'air_temperature_nc'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Scale the data before imputation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 3\u001b[0m scaled_data \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimpute_vars\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Apply IterativeImputer (MICE)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m imputer \u001b[38;5;241m=\u001b[39m IterativeImputer(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/geoscience_course/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/geoscience_course/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/geoscience_course/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['RH', 'precip_int_h_D_nc', 'air_pressure_nc', 'wind_dir_nc', 'specific_humidity_nc', 'wind_speed_nc', 'R_LW_in_corr_nc', 'R_SW_in_nc', 'air_temperature_nc'] not in index\""
     ]
    }
   ],
   "source": [
    "# Scale the data before imputation\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(merged_df[impute_vars])\n",
    "\n",
    "# Apply IterativeImputer (MICE)\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "imputed_data = imputer.fit_transform(scaled_data)\n",
    "\n",
    "# Inverse transform to return to original scale\n",
    "imputed_data = scaler.inverse_transform(imputed_data)\n",
    "\n",
    "# Update the DataFrame with the imputed values\n",
    "for i, var in enumerate(impute_vars):\n",
    "    merged_df[var] = imputed_data[:, i]\n",
    "\n",
    "# If needed, separate point observations and reanalysis data after imputation\n",
    "df_filled = merged_df[['timestamp'] + vars_df]\n",
    "nc_filled = merged_df[['timestamp'] + [var + '_nc' for var in vars_df]]\n",
    "\n",
    "print(\"Imputed Point Observations Data:\")\n",
    "print(df_filled.head())\n",
    "\n",
    "print(\"\\nImputed NetCDF Data:\")\n",
    "print(nc_filled.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoscience_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
