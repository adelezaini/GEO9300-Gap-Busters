{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported libraries\n",
      "\n",
      "Machine Learning method: linear_regression\n",
      "Evaluation metrics: ['r2', 'mse', 'mae']\n"
     ]
    }
   ],
   "source": [
    "# Description: This script is used to create a workflow for the data analysis\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the parent directory and add it to sys.path\n",
    "#parent_dir = os.path.abspath(\"/notebooks/workflow_Eivind\")\n",
    "#sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Now import the module\n",
    "from workflow_Eivind.ML_lib4 import *\n",
    "\n",
    "# packages:\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from tensorflow.keras import Sequential\n",
    "#from tensorflow.keras.layers import Input, Dense, Dropout #, LSTM\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "#from bartpy.sklearnmodel import SklearnModel as BART\n",
    "\n",
    "print(\"Imported libraries\\n\")\n",
    "\n",
    "# Script workflow:\n",
    "# - load data\n",
    "# - scaling\n",
    "# - split into training and testing based on data gaps\n",
    "# - machine learning models and applying hyperparameter search grids\n",
    "\n",
    "#### to change: import from external argument ###\n",
    "algorithm = 'linear_regression' #look below for the options\n",
    "scoring_metrics = ['r2','mse','mae']\n",
    "\n",
    "print(f\"Machine Learning method: {algorithm}\")\n",
    "print(f\"Evaluation metrics: {scoring_metrics}\")\n",
    "\n",
    "# save hyperparameters for the run as csv file?\n",
    "save_to_csv = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(dataframe, algorithm, cyclic_features=None, verbose=False, plot=False):\n",
    "\n",
    "    scaling_method = None\n",
    "\n",
    "    assert algorithm in [\"linear_regression\", \"neural_network\", \"bart\", \"lstm\", \"random_forest\", \"xgboost\"], \\\n",
    "        \"Invalid algorithm specified.\"\n",
    "\n",
    "    # Determine scaling method based on algorithm\n",
    "    if algorithm in [\"lstm\", \"neural_network\"]:\n",
    "        scaling_method = \"minmax\"\n",
    "        if verbose: print(f\"Algorithm '{algorithm}' selected: Using MinMax scaling.\")\n",
    "    elif algorithm == \"linear_regression\":\n",
    "        scaling_method = \"standard\"\n",
    "        if verbose: print(f\"Algorithm '{algorithm}' selected: Using Standard scaling.\")\n",
    "    else:\n",
    "        scaling_method = None\n",
    "        if verbose: print(f\"Algorithm '{algorithm}' selected: No scaling will be applied.\")\n",
    "\n",
    "    scaled_df = dataframe.copy()\n",
    "    scaling_info = {}\n",
    "    columns = [col for col in dataframe.columns]\n",
    "\n",
    "    if verbose: print(\"Scaling in progress...\\n\")\n",
    "    for col in columns:\n",
    "        if verbose: print(f\"Processing column: {col}\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        if scaled_df[col].isnull().any():\n",
    "            if verbose: print(f\"Warning: Column '{col}' contains missing values. Imputing with median.\")\n",
    "            scaled_df[col].fillna(scaled_df[col].median(), inplace=True)\n",
    "\n",
    "        # Cyclical encoding for specified features\n",
    "        if cyclic_features and col in cyclic_features:\n",
    "            max_value = scaled_df[col].max() + 1  # Assuming cyclic range [0, max_value - 1]\n",
    "            if verbose: print(f\"  Applying cyclical encoding (sine/cosine) for '{col}'.\")\n",
    "            scaled_df[f\"{col}_sin\"] = np.sin(2 * np.pi * scaled_df[col] / max_value)\n",
    "            scaled_df[f\"{col}_cos\"] = np.cos(2 * np.pi * scaled_df[col] / max_value)\n",
    "            scaling_info[col] = {\"method\": \"cyclical_encoding\"}\n",
    "            scaled_df = scaled_df.drop(columns=col)\n",
    "            continue\n",
    "\n",
    "        # Skip scaling if no scaling method is selected\n",
    "        if scaling_method is None:\n",
    "            if verbose: print(f\"No scaling applied for column: {col}\")\n",
    "            scaling_info[col] = {\"method\": \"none\"}\n",
    "            continue\n",
    "\n",
    "        # Determine the scaler\n",
    "        if scaling_method == \"minmax\":\n",
    "            scaler = MinMaxScaler()\n",
    "            message = \"Applying MinMaxScaler.\"\n",
    "        elif scaling_method == \"standard\":\n",
    "            scaler = StandardScaler()\n",
    "            message = \"Applying StandardScaler.\"\n",
    "        elif scaling_method == \"logminmax\":\n",
    "            scaled_df[col] = np.log1p(scaled_df[col] - scaled_df[col].min() + 1)\n",
    "            scaler = MinMaxScaler()\n",
    "            message = \"Applying log transformation followed by MinMaxScaler.\"\n",
    "        elif scaling_method == \"individual\":\n",
    "            skewness = skew(scaled_df[col])\n",
    "            _, p_value = normaltest(scaled_df[col])\n",
    "            if verbose: print(f\"  Skewness: {skewness:.2f}, Normality test p-value: {p_value:.4f}\")\n",
    "            if p_value > 0.05:  # Normally distributed\n",
    "                scaler = StandardScaler()\n",
    "                message = \"Applying StandardScaler (data is approximately normal).\"\n",
    "            elif abs(skewness) > 1:  # Highly skewed\n",
    "                scaled_df[col] = np.log1p(scaled_df[col] - scaled_df[col].min() + 1)\n",
    "                scaler = MinMaxScaler()\n",
    "                message = \"Applying log transformation followed by MinMaxScaler (data is skewed).\"\n",
    "            else:  # Mildly skewed or uniform\n",
    "                scaler = MinMaxScaler()\n",
    "                message = \"Applying MinMaxScaler (data is mildly skewed or uniform).\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scaling method: {scaling_method}\")\n",
    "\n",
    "        if verbose: print(message)\n",
    "        \n",
    "        # Apply scaling\n",
    "        scaled_values = scaler.fit_transform(scaled_df[col].values.reshape(-1, 1))\n",
    "        scaled_df[col] = scaled_values.flatten()\n",
    "        scaling_info[col] = {\"method\": type(scaler).__name__}\n",
    "\n",
    "        if scaling_method == \"individual\":\n",
    "            scaling_info[col].update({\n",
    "                \"skewness\": skewness,\n",
    "                \"normality_p_value\": p_value,\n",
    "            })\n",
    "\n",
    "        if plot: plot_scaling(dataframe, scaled_df, col)\n",
    "\n",
    "    return scaled_df, scaling_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "### Workflow Parameters\n",
    "################################################################################\n",
    "\n",
    "# Define features to run as input variables for the models:\n",
    "features = [\n",
    "    \"SWin\",\n",
    "    \"LWin\",\n",
    "    \"Tair\",\n",
    "    \"RH_air\",\n",
    "    \"prec\",\n",
    "    \"u\",\n",
    "    \"snow_cover\",\n",
    "    \"hour\",\n",
    "    \"doy\"\n",
    "    ]\n",
    "\n",
    "# Choose the gaps dataset - either structured or random gaps\n",
    "gaps_data_file = 'random_gaps_1' # 'random_gaps_1' -- values from 1 to 5 for diff versions\n",
    "\n",
    "# Define the cross-validation strategy:\n",
    "CV_scoring = 'neg_mean_absolute_error'   # e.g. 'neg_mean_squared_error', 'r2', 'neg_root_mean_squared_error. More available methods for regression evaluation (scoring): https://scikit-learn.org/1.5/modules/model_evaluation.html#scoring-parameter)\n",
    "cv = 3  # Number of cross-validation folds\n",
    "\n",
    "################################################################################\n",
    "### Data\n",
    "################################################################################\n",
    "\n",
    "\n",
    "##### Load the synthetic dataset:\n",
    "# a. Load single CSV files in separate dfs\n",
    "# b. Merge the dfs into one single \"synthetic_dataset\"\n",
    "\n",
    "folder_path = '../data/synthetic_dataset' # NOTE: May need to adjust if the script is used from another folder\n",
    "\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv')) # use glob library to find all CSV files\n",
    "\n",
    "dfs = [] #to store individual DataFrames.\n",
    "\n",
    "for file in csv_files:\n",
    "    data = pd.read_csv(file, parse_dates=['time'], sep=',')\n",
    "    # 'parse_dates' argument ensures the 'time' column is interpreted as datetime objects.\n",
    "    \n",
    "    dfs.append(data)\n",
    "\n",
    "syn_ds = dfs[0] # Start with the first DataFrame as the base for merging.\n",
    "\n",
    "for data in dfs[1:]:\n",
    "    # Merge each subsequent DataFrame with the base DataFrame (`syn_ds`).\n",
    "    # The merge is done using an ordered merge on the 'time' column.\n",
    "    # This ensures that the merged dataset remains sorted by 'time'.\n",
    "    syn_ds = pd.merge_ordered(syn_ds, data, on='time')\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Features and target variables:\n",
    "\n",
    "syn_ds[\"time\"] = pd.to_datetime(syn_ds[\"time\"])\n",
    "syn_ds[\"doy\"] = syn_ds[\"time\"].dt.dayofyear\n",
    "syn_ds[\"hour\"] = syn_ds[\"time\"].dt.hour\n",
    "\n",
    "y = syn_ds[\"LE\"]\n",
    "X = syn_ds[features]\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Split into training and testing datasets based on gaps in LE:\n",
    "\n",
    "def split_train_test_dataset(original_X, original_y):\n",
    "    # Function to load data gaps dataset\n",
    "    def load_data_gaps(file_name):\n",
    "        return pd.read_csv(f'../data/LE-gaps/{file_name}.csv', parse_dates=['time'], sep=',') # NOTE: May need to adjust if the script is used from another folder\n",
    "\n",
    "    LE_gaps = load_data_gaps(gaps_data_file)\n",
    "\n",
    "    # Select X and y where LE_gaps is not null\n",
    "    X_train = original_X[LE_gaps['LE_gaps'].notnull()]\n",
    "    y_train = original_y[LE_gaps['LE_gaps'].notnull()]\n",
    "\n",
    "    # The following test set is for where there are data gaps\n",
    "    X_test = original_X[LE_gaps['LE_gaps'].isnull()]\n",
    "\n",
    "    # LE without gaps:\n",
    "    LE = syn_ds['LE']\n",
    "    # extract LE where LE_gaps is null:\n",
    "    y_test = LE[LE_gaps['LE_gaps'].isnull()]\n",
    "\n",
    "    print(\"Created training and testing datasets\\n\")\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, scoring):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on the test data and compute specified metrics.\n",
    "\n",
    "    Parameters:\n",
    "        model (object): Trained machine learning model.\n",
    "        X_test (array-like): Test feature matrix.\n",
    "        y_test (array-like): Test target vector.\n",
    "        scoring (list, optional): List of scoring metrics for evaluation (default is ['r2']).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing predictions and evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Validate the scoring parameter\n",
    "    valid_metrics = {'r2', 'mse', 'mae'}\n",
    "    invalid_metrics = [metric for metric in scoring if metric not in valid_metrics]\n",
    "    if invalid_metrics:\n",
    "        raise ValueError(f\"Invalid scoring metric(s): {invalid_metrics}. Allowed values are {valid_metrics}.\")\n",
    "    \n",
    "    try:\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Compute evaluation metrics\n",
    "        metrics = {}\n",
    "        print(f\"Test Metrics:\")\n",
    "        if 'r2' in scoring:\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            metrics['r2'] = r2\n",
    "            print(f\"  R² Score: {r2:.2f}\")\n",
    "        if 'mse' in scoring:\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            metrics['mse'] = mse\n",
    "            print(f\"  Mean Squared Error: {mse:.2f}\")\n",
    "        if 'mae' in scoring:\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            metrics['mae'] = mae\n",
    "            print(f\"  Mean Absolute Error: {mae:.2f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during model evaluation: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training and testing datasets\n",
      "\n",
      "Test Metrics:\n",
      "  R² Score: 0.75\n",
      "  Mean Squared Error: 957.09\n",
      "  Mean Absolute Error: 22.64\n",
      "\n",
      "=== LINEAR REGRESSION RESULTS ===\n",
      "Test Metrics: {'r2': 0.7514328851756489, 'mse': np.float64(957.0945303337479), 'mae': np.float64(22.64125084110847)}\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# MACHINE LEARNING MODELS\n",
    "################################################################################\n",
    "\n",
    "def machine_learning_method(algorithm):\n",
    "\n",
    "    allowed_algorithms = {'linear_regression', 'random_forest', 'neural_network', 'lstm', 'xgboost', 'bart'}\n",
    "    \n",
    "    # Validate the algorithm\n",
    "    assert algorithm in allowed_algorithms, (\n",
    "        f\"Invalid algorithm '{algorithm}'. \"\n",
    "        f\"Allowed values are: {', '.join(allowed_algorithms)}.\"\n",
    "    )\n",
    "    \n",
    "    if algorithm == 'linear_regression':\n",
    "\n",
    "        # LINEAR REGRESSION (No hyperparameters to tune)\n",
    "        \n",
    "        # Apply scaling to input features\n",
    "        X_scaled, _ = scaling(X, algorithm, cyclic_features = ['hour','doy'])\n",
    "        \n",
    "        # Split training and testing datasets\n",
    "        X_train, y_train, X_test, y_test = split_train_test_dataset(X_scaled, y)\n",
    "        \n",
    "        # Apply machine learning method\n",
    "        lr_model = LinearRegression()\n",
    "        lr_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Generate predictions\n",
    "        y_pred = lr_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the ML method\n",
    "        LR_metrics = evaluate_model(lr_model, X_test, y_test,scoring_metrics)\n",
    "        metric = LR_metrics # for the general saving code at the end of the script\n",
    "\n",
    "        print(\"\\n=== LINEAR REGRESSION RESULTS ===\")\n",
    "        print(f\"Test Metrics: {LR_metrics}\")\n",
    "\n",
    "        #-------------------------------------------------------------------------------\n",
    "    elif algorithm == 'random_forest':\n",
    "\n",
    "        # RANDOM FOREST REGRESSOR\n",
    "        \n",
    "        # Split training and testing datasets\n",
    "        X_train, y_train, X_test, y_test = split_train_test_dataset(X, y)\n",
    "        \n",
    "        param_grid_rf = {\n",
    "            'n_estimators': [100, 200, 300, 500],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4, 8]\n",
    "        }\n",
    "\n",
    "        rf_model = RandomForestRegressor()\n",
    "        RF_best_model, RF_best_params, cv_results = model_tuning_CV(X_train, y_train, rf_model, param_grid_rf, cv, CV_scoring)\n",
    "        RF_metrics = evaluate_model(RF_best_model, X_test, y_test, scoring_metrics)\n",
    "        metric = RF_metrics # for the general saving code at the end of the script\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = RF_best_model.predict(X_test)\n",
    "\n",
    "        print(\"\\n=== RANDOM FOREST RESULTS ===\")\n",
    "        print(f\"Best Parameters: {RF_best_params}\")\n",
    "        print(f\"Test Metrics: {RF_metrics}\")\n",
    "\n",
    "        #-------------------------------------------------------------------------------\n",
    "    elif algorithm == 'neural_network':\n",
    "    \n",
    "        # NEURAL NETWORK REGRESSOR\n",
    "        \n",
    "        # Apply scaling to input features\n",
    "        X_scaled, _ = scaling(X, algorithm, cyclic_features = ['hour','doy'])\n",
    "        \n",
    "        # Split training and testing datasets\n",
    "        X_train, y_train, X_test, y_test = split_train_test_dataset(X_scaled, y)\n",
    "        \n",
    "        def create_NN_model(units=64, activation='relu'):\n",
    "            model = Sequential([\n",
    "                Input(shape=(X_train.shape[1],)),\n",
    "                Dense(units, activation=activation, kernel_initializer='uniform'),\n",
    "                Dropout(0.2),\n",
    "                Dense(units, activation=activation),\n",
    "                Dropout(0.2),\n",
    "                Dense(1)\n",
    "            ])\n",
    "            model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "            return model\n",
    "\n",
    "        keras_model = KerasRegressor(model=create_NN_model, verbose=0)\n",
    "\n",
    "        param_grid_nn = {\n",
    "            'model__units': [32, 64, 128],\n",
    "            'model__activation': ['relu', 'tanh', 'sigmoid'],\n",
    "            'batch_size': [10, 20, 30],\n",
    "            'epochs': [50, 100, 200]\n",
    "        }\n",
    "\n",
    "        NN_best_model, NN_best_params, cv_results = model_tuning_CV(X_train, y_train, keras_model, param_grid_nn, cv, CV_scoring)\n",
    "        NN_metrics = evaluate_model(NN_best_model, X_test, y_test, scoring_metrics)\n",
    "        metric = NN_metrics # for the general saving code at the end of the script\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = NN_best_model.predict(X_test)\n",
    "\n",
    "        print(\"\\n=== NEURAL NETWORK RESULTS ===\")\n",
    "        print(f\"Best Parameters: {NN_best_params}\")\n",
    "        print(f\"Test Metrics: {NN_metrics}\")\n",
    "\n",
    "        #-------------------------------------------------------------------------------\n",
    "    elif algorithm == 'lstm':\n",
    "    \n",
    "        # LSTM REGRESSOR\n",
    "        \n",
    "        # Apply scaling to input features\n",
    "        X_scaled, _ = scaling(X, algorithm, cyclic_features = ['hour','doy'])\n",
    "        \n",
    "        # Split training and testing datasets\n",
    "        X_train, y_train, X_test, y_test = split_train_test_dataset(X_scaled, y)\n",
    "        \n",
    "        \n",
    "        def create_lstm_model(units=64, activation='relu'):\n",
    "            model = Sequential([\n",
    "                Input(shape=(X_train.shape[1], 1)),\n",
    "                LSTM(units, activation=activation, return_sequences=False),\n",
    "                Dense(1)\n",
    "            ])\n",
    "            model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "            return model\n",
    "\n",
    "        lstm_model = KerasRegressor(model=create_lstm_model, verbose=0)\n",
    "\n",
    "        param_grid_lstm = {\n",
    "            'model__units': [32, 64, 128],\n",
    "            'model__activation': ['relu', 'tanh'],\n",
    "            'batch_size': [10, 20],\n",
    "            'epochs': [50, 100]\n",
    "        }\n",
    "\n",
    "        LSTM_best_model, LSTM_best_params, cv_results = model_tuning_CV(X_train, y_train, lstm_model, param_grid_lstm, cv, CV_scoring)\n",
    "        LSTM_metrics = evaluate_model(LSTM_best_model, X_test, y_test, scoring_metrics)\n",
    "        metric = LSTM_metrics # for the general saving code at the end of the script\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = LSTM_best_model.predict(X_test)\n",
    "\n",
    "        print(\"\\n=== LSTM RESULTS ===\")\n",
    "        print(f\"Best Parameters: {LSTM_best_params}\")\n",
    "        print(f\"Test Metrics: {LSTM_metrics}\")\n",
    "\n",
    "    #-------------------------------------------------------------------------------\n",
    "    elif algorithm == 'xgboost':\n",
    "    \n",
    "        # XGBOOST REGRESSOR\n",
    "        \n",
    "        # Apply scaling to input features\n",
    "        X_scaled, _ = scaling(X, algorithm, cyclic_features = ['hour','doy'])\n",
    "        \n",
    "        # Split training and testing datasets\n",
    "        X_train, y_train, X_test, y_test = split_train_test_dataset(X_scaled, y)\n",
    "\n",
    "        param_grid_xgb = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0]\n",
    "        }\n",
    "\n",
    "        xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "        XGB_best_model, XGB_best_params, cv_results = model_tuning_CV(X_train, y_train, xgb_model, param_grid_xgb, cv, CV_scoring)\n",
    "        XGB_metrics = evaluate_model(XGB_best_model, X_test, y_test, scoring_metrics)\n",
    "        metric = XGB_metrics # for the general saving code at the end of the script\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = XGB_best_model.predict(X_test)\n",
    "\n",
    "        print(\"\\n=== XGBOOST RESULTS ===\")\n",
    "        print(f\"Best Parameters: {XGB_best_params}\")\n",
    "        print(f\"Test Metrics: {XGB_metrics}\")\n",
    "\n",
    "    #-------------------------------------------------------------------------------\n",
    "    elif algorithm == 'bart':\n",
    "    \n",
    "        # BART REGRESSOR\n",
    "        \n",
    "        # Apply scaling to input features\n",
    "        X_scaled, _ = scaling(X, algorithm, cyclic_features = ['hour','doy'])\n",
    "        \n",
    "        # Split training and testing datasets\n",
    "        X_train, y_train, X_test, y_test = split_train_test_dataset(X_scaled, y)\n",
    "\n",
    "        bart_model = BART(random_state=42)\n",
    "        \n",
    "        param_grid_bart = {\n",
    "            'n_trees': [50, 100, 200],\n",
    "            'alpha': [0.95, 0.99],\n",
    "            'beta': [1.0, 2.0],\n",
    "            'k': [2.0, 3.0]\n",
    "        }\n",
    "\n",
    "        BART_best_model, BART_best_params, cv_results = model_tuning_CV(X_train, y_train, bart_model, param_grid_bart, cv, CV_scoring)\n",
    "        BART_metrics = evaluate_model(BART_best_model, X_test, y_test, scoring_metrics)\n",
    "        metric = BART_metrics # for the general saving code at the end of the script\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = bart_model.predict(X_test)\n",
    "\n",
    "        print(\"\\n=== BART RESULTS ===\")\n",
    "        print(f\"Best Parameters: {BART_best_params}\")\n",
    "        print(f\"Test Metrics: {BART_metrics}\")\n",
    "#-------------------------------------------------------------------------------\n",
    "    # SAVING RESULTS\n",
    "    \n",
    "    # Save results to CSV\n",
    "\n",
    "    if save_to_csv and algorithm != 'linear_regression':\n",
    "    # Save CV results\n",
    "        cv_results_df = pd.DataFrame(cv_results)\n",
    "        cv_results_df.to_csv(f'../results/{algorithm}_{gaps_data_file}_cv_results.csv', index=False)\n",
    "\n",
    "    if save_to_csv:\n",
    "    # Save predictions\n",
    "        results_df = pd.DataFrame({\n",
    "            'index': X_test.index,\n",
    "            'Actual': y_test,\n",
    "            'Predicted': y_pred\n",
    "        })\n",
    "        results_df.to_csv(f'../results/{algorithm}_{gaps_data_file}_predictions.csv', index=False)\n",
    "\n",
    "    # Save R² or MSE value if available\n",
    "    if scoring_metrics != None:\n",
    "\n",
    "        # Prepare the DataFrame with all metrics\n",
    "        metric_df = pd.DataFrame([metric])  # Convert the dictionary into a DataFrame\n",
    "\n",
    "        # Define the output file path\n",
    "        metric_file_name = f'../results/{algorithm}_{gaps_data_file}_metrics_results.csv'\n",
    "\n",
    "        # Save metrics to the CSV file\n",
    "        metric_df.to_csv(metric_file_name, index=False)\n",
    "\n",
    "        #if scoring_metrics == 'r2':\n",
    "        #    metric_df = pd.DataFrame({'R2_Score': [metric]})\n",
    "        #    metric_file_name = f'../results/{algorithm}_{gaps_data_file}_r2_results.csv'\n",
    "        #elif scoring_metrics == 'mse':\n",
    "        #    metric_df = pd.DataFrame({'MSE': [metric]})\n",
    "        #    metric_file_name = f'../results/{algorithm}_{gaps_data_file}_mse_results.csv'\n",
    "\n",
    "        #if scoring_metrics is not None:\n",
    "        #    metric_df.to_csv(metric_file_name, index=False)\n",
    "\n",
    "# Perform the function\n",
    "machine_learning_method(algorithm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
